\documentclass[journal]{IEEEtran}

% --- PAQUETES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish, es-tabla]{babel} % Soporte para español
\usepackage{cite}      % Citas estilo IEEE
\usepackage{amsmath, amssymb, amsfonts} % Matemáticas
\usepackage{algorithmic}
\usepackage{graphicx}  % Gráficos
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}

% --- DEFINICIÓN DE DATOS ---
\title{Diseño y evaluación de un sistema DSP para la rehabilitación vocal post-cordectomía mediante reconstrucción espectral e IA}

% Formato de autores estilo IEEE
\author{Alfonso~Gamboa~Rubén y~Flores~Montero~Edsel~Yetlanezi%
\thanks{Los autores son estudiantes de Ingeniería Biomédica.}%
\thanks{Manuscrito generado el \today.}}

% --- DOBLE RESUMEN (Hack para IEEE) ---
% Usamos este comando especial de IEEEtran para meter contenido
% a doble ancho antes de que empiecen las columnas normales.
\IEEEtitleabstractindextext{%
    \begin{minipage}[t]{0.48\textwidth}
        \selectlanguage{spanish}
        \textbf{\textit{Resumen}—} 
        La cordectomía, procedimiento quirúrgico que implica la extirpación parcial o total de los pliegues vocales, compromete severamente la capacidad comunicativa del paciente, afectando su identidad y calidad de vida. Este proyecto presenta el diseño y evaluación de un sistema de procesamiento digital de señales (DSP) orientado a la rehabilitación vocal no invasiva mediante la reconstrucción espectral. La metodología evolucionó a través de tres fases iterativas: una aproximación inicial en el dominio de la frecuencia (FFT global), un modelo adaptativo basado en metadatos y filtrado de intensidad adaptable, y finalmente, la implementación basada en la Transformada de Fourier de Tiempo Corto (STFT) y estimadores estadísticos (MMSE-STSA). Los resultados experimentales demostraron que, si bien la sustracción de ruido estacionario mediante algoritmos de Wiener y Ephraim-Malah es efectiva, la reconstrucción de la voz requiere una intervención más compleja a nivel de las micro-características que forman la voz para preservar la identidad del paciente.
        
        \smallskip
        \textbf{\textit{Palabras Clave}—} DSP, STFT, Filtro de Wiener, Savitzky-Golay, VAD, Análisis Espectral, Rehabilitación Fónica, Cordectomía, IA, RLHF, Red Neuronal, SER.
    \end{minipage}
    \hfill % Espacio flexible en medio
    \begin{minipage}[t]{0.48\textwidth}
        \selectlanguage{english}
        \textbf{\textit{Abstract}—} 
        Cordectomy, a surgical procedure involving the partial or total removal of the vocal folds, severely compromises the patient's communicative capacity, affecting their identity and quality of life. This project presents the design and evaluation of a digital signal processing (DSP) system oriented towards non-invasive vocal rehabilitation through spectral reconstruction. The methodology evolved through three iterative phases: an initial frequency-domain approach (global FFT), an adaptive model based on metadata and adaptive intensity filtering, and finally, an implementation based on the Short-Time Fourier Transform (STFT) and statistical estimators (MMSE-STSA). Experimental results demonstrated that while stationary noise subtraction using Wiener and Ephraim-Malah algorithms is effective, voice reconstruction requires a more complex intervention at the level of the micro-characteristics that form the voice to preserve patient identity.
        
        \smallskip
        \textbf{\textit{Keywords}—} DSP, STFT, Wiener Filter, Savitzky-Golay, VAD, Spectral Analysis, Phonic Rehabilitation, Cordectomy, AI, RLHF, Neural Network, SER.
    \end{minipage}
}

\begin{document}

\maketitle

% Aseguramos que el texto principal empiece en español
\selectlanguage{spanish}

% --- SECCIÓN 1: OBJETIVOS ---
\section{Objetivos del Proyecto}

\subsection{Objetivo General}
Desarrollar y evaluar algoritmos de procesamiento digital de señales basado en análisis espectral de tiempo corto y modelado estadístico, en relación a la capacidad de mejorar la calidad de la voz y restaurar parcialmente las características tímbricas en grabaciones de voz de pacientes sometidos a cordectomía.

\subsection{Objetivos Específicos}
\begin{enumerate}
    \item \textbf{Caracterización Acústica:} Construir una base de datos pareada (pre y post-operatoria) para identificar los patrones de pérdida armónica y deformación espectral en el dominio de la frecuencia causados por la intervención quirúrgica.
    \item \textbf{Optimización de la Relación Señal-Ruido (SNR):} Implementar y comparar técnicas de sustracción espectral (Noisereduce vs. Ephraim-Malah/VAD) para minimizar el ruido estacionario inherente a la fonación soplada sin degradar los transitorios de la voz.
    \item \textbf{Reconstrucción Espectral:} Experimentar con algoritmos de transferencia de características que utilicen una máscara espectral diferencial ($T_{dB}$) para proyectar el timbre e identidad del sonido vocal (envolvente de frecuencia de la voz) sano sobre la señal patológica.
    \item \textbf{Validación Técnica:} Evaluar mediante espectrogramas y gráficas comparativas, la efectividad de los algoritmos en la rehabilitación de formantes y reducción de artefactos y desfase de frecuencias armónicas.
\end{enumerate}

% --- SECCIÓN 2: MARCO TEÓRICO ---
\section{Marco Teórico}

\subsection{Software y Herramientas}

\subsubsection{Lenguaje de Programación: Python}
Para la implementación de los algoritmos de procesamiento de audio, se seleccionó Python como lenguaje núcleo. Esta elección se basa en la extensa documentación y la robustez de su ecosistema de librerías científicas (\textit{SciPy Stack}, etc.), que permiten prototipar y desplegar soluciones complejas matemáticas, estadísticas y procesamiento de señales con alta eficiencia \cite{ref1, ref2}.

\subsubsection{Librerías Especializadas}
\begin{itemize}
    \item \textbf{Numpy (\texttt{numpy}):} Fundamental para la manipulación de arreglos multidimensionales \cite{ref3}. Se utiliza para convertir los flujos de bits de audio en arreglos de punto flotante (\texttt{float32}) \cite{ref4}.
    \item \textbf{Pydub (\texttt{pydub}):} Interfaz de alto nivel para el manejo de archivos de audio (I/O).
    \item \textbf{Scipy (\texttt{scipy.signal}):} Proporciona herramientas matemáticas avanzadas \cite{ref5, ref6}.
    \begin{itemize}
        \item \textit{Filtro Savitzky-Golay:} Utilizado para el suavizado de curvas espectrales \cite{ref7, ref8}. Ajusta un polinomio de orden $k$ a una ventana de puntos $m$ mediante mínimos cuadrados \cite{ref9}, preservando los formantes \cite{ref10}.
        Su formulación discreta es:
        \begin{equation}
            Y_j = \sum_{i=-(m-1)/2}^{(m-1)/2} C_i \cdot y_{j+i}
        \end{equation}
        Donde $Y_j$ es el valor suavizado, $y$ los datos crudos, $m$ el tamaño de la ventana y $C_i$ los coeficientes \cite{ref11}.
    \end{itemize}
\end{itemize}

\subsection{Fundamentos Matemáticos}

\subsubsection{STFT}
La voz es una señal no estacionaria, por lo que la Transformada de Fourier clásica es insuficiente \cite{ref14}. La STFT divide la señal en ventanas temporales \cite{ref15}:
\begin{equation}
    X(m, k) = \sum_{n=0}^{N-1} x(n + mH) w(n) e^{-j \frac{2\pi}{N} kn}
\end{equation}
Donde $m$ es el índice temporal, $k$ el índice de frecuencia y $H$ el tamaño del salto \cite{ref16, ref17}.

\subsubsection{Algoritmo de Filtrado: Wiener}
El filtro de Wiener calcula una ganancia óptima $W(f)$ basada en la SNR \cite{ref22, ref23}:
\begin{equation}
    W(f) = \frac{P_{se\tilde{n}al}(f)}{P_{se\tilde{n}al}(f) + P_{ruido}(f)}
\end{equation}

\subsection{Conceptos Estadísticos}
\subsubsection{Desviación Estándar ($\sigma$)}
Un píxel espectral se considera "señal" si supera un umbral dinámico:
\begin{equation}
    \text{Umbral}(f) = \mu_{ruido}(f) + (n \cdot \sigma_{ruido}(f))
\end{equation}

\subsection{Acústica de la Voz}
\subsubsection{Los Formantes}
Picos de resonancia espectral \cite{ref28, ref29}:
\begin{itemize}
    \item \textbf{F1 y F2:} Determinan la vocal \cite{ref30}.
    \item \textbf{F3, F4 y F5:} Determinan el timbre e identidad \cite{ref34, ref35}.
\end{itemize}

\subsection{Cordectomía y Modelo Fuente-Filtro}
La cordectomía reseca las cuerdas vocales por neoplasias \cite{ref36, ref37}, causando disfonía \cite{ref38}. El modelo acústico estándar (Fant, 1960) \cite{ref43} separa la Fuente (vibración) del Filtro (tracto vocal) \cite{ref46}.

% --- SECCIÓN 3: METODOLOGÍA ---
\section{Metodología}

\subsection{Versión 1.0: Análisis Espectral}

\subsubsection{Algoritmo 1.1.0: Preprocesamiento}
Sea $x(n)$ la señal de entrada, su representación en frecuencia $X(k)$ se define como:
\begin{equation}
    X(k) = \sum_{n=0}^{N-1} x(n) e^{-j \frac{2\pi}{N} k n}
\end{equation}

% NOTA: Usamos figure (sin asterisco) para que ocupe solo una columna
\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth} % Usamos columnwidth
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 1}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 1.1.0}
    \label{fig:m1}
\end{figure}

\subsubsection{Algoritmo 1.2.0: Rehabilitación}
Se calcula un factor de ganancia $G(k)$:
\begin{equation}
    G(k) = \frac{|X_{pre}(k)|}{|X_{post}(k)|}
\end{equation}
Señal rehabilitada $Y_{reh}(k)$:
\begin{equation}
    Y_{reh}(k) = X_{post}(k) \cdot G(k)
\end{equation}
Aplicación de la IFFT:
\begin{equation}
    y(n) = \frac{1}{N} \sum_{k=0}^{N-1} Y_{reh}(k) e^{j \frac{2\pi}{N} k n}
\end{equation}

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 2}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 1.2.0}
\end{figure}

\subsubsection{Algoritmo 1.1.1: Archivos Independientes}
Iteración para procesamiento unilateral.

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 3}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 1.1.1}
\end{figure}

\subsubsection{Algoritmo 1.2.1: Suma Diferencial}
Compensación aditiva basada en promedios espectrales.
\begin{equation}
    \Delta_{media}(k) = \mu_{pre}(k) - \mu_{post}(k)
\end{equation}
\begin{equation}
    |Y_{rehab}(k)| = |X_{post}(k)| + \Delta_{media}(k)
\end{equation}

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 4}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 1.2.1}
\end{figure}

\subsubsection{Algoritmo 1.2.2: Inyección Proyectada}
Control estadístico para evitar distorsión usando $\mu$ y $\sigma$.
\begin{equation}
    G_{corr}(k) = \frac{|X_{post}(k)| + I_{proy}(k)}{\mu_{pre}(k)}
\end{equation}

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 5}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 1.2.2}
\end{figure}

\subsection{Versión 2.0: Metadatos}

\subsubsection{Algoritmo 2.1.0: Filtrado Selectivo}
Uso de REGEX para configuración dinámica de filtros.

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 6}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 2.1.0}
\end{figure}

\subsubsection{Algoritmo 2.2.1.0: Modelo Espectral}
Promedio ponderado ($w$) según calidad de grabación.
\begin{equation}
    S_{ideal}(k) = \frac{\sum_{i=1}^{N} (X_i(k) \cdot w_i)}{\sum_{i=1}^{N} w_i}
\end{equation}

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 7}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 2.2.1.0}
\end{figure}

\subsubsection{Algoritmo 2.2.2.0: Reconstrucción Híbrida}
Sustitución espectral en banda alta y amplificación en media, suavizado con Savitzky-Golay.

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 8}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 2.2.2.0}
\end{figure}

\subsection{Versión 3.0: Estimación MMSE}

\subsubsection{Algoritmo 3.1.0: MMSE-STSA con VAD}
Estimador de Ephraim-Malah con Detección de Actividad de Voz.

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 9}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 3.1.0}
\end{figure}

\subsubsection{Algoritmo 3.2.0: Visualización}
Generación de espectrogramas logarítmicos.
\begin{equation}
    S(m, k) = 10 \cdot \log_{10}(|X(m, k)|^2)
\end{equation}

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 10}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 3.2.0}
\end{figure}

\subsubsection{Algoritmo 3.3.0: Máscara de Transferencia}
Definición de Función de Transferencia Objetivo ($T_{dB}$):
\begin{equation}
    T_{dB}(k) = \mu_{PRE, dB}(k) - \mu_{POST, dB}(k)
\end{equation}
Aplicación a la señal post-operatoria:
\begin{equation}
    |Y_{reh}(m, k)| = |X_{post}(m, k)| \cdot 10^{\frac{T_{dB}(k)}{20}}
\end{equation}

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 11}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo del Algoritmo 3.3.0}
\end{figure}

\subsection{Versión 4.0: IA (Propuesta)}

% NOTA: Usamos figure* (con asterisco) para diagramas complejos
% Esto hace que la imagen ocupe AMBAS columnas (ancho de página)
\begin{figure*}[htbp]
    \centering
    \fbox{\begin{minipage}{0.8\textwidth} % ancho del 80% de la pagina
    \centering
    \vspace{1cm}
    \textbf{Aquí va mermaid 12 (Diagrama Fase 1)}
    \vspace{1cm}
    \end{minipage}}
    \caption{Diagrama de flujo de la Fase 1: Reconstrucción Offline}
\end{figure*}

\subsubsection{Fase 1: Reconstrucción Offline}
Arquitectura ASR-TTS (Whisper + XTTS).
\begin{equation}
    P(Y | T, S) = \prod_{n} P(y_n | y_{<n}, T, S)
\end{equation}

\begin{figure*}[htbp]
    \centering
    \fbox{\begin{minipage}{0.8\textwidth}
    \centering
    \vspace{1cm}
    \textbf{Aquí va mermaid 13 (Diagrama Fase 2)}
    \vspace{1cm}
    \end{minipage}}
    \caption{Diagrama de flujo de la Fase 2: Optimización de Preferencias}
\end{figure*}

\subsubsection{Fase 2: Optimización de Preferencias}
Ajuste de vectores de estilo (RLHF simplificado).

\subsubsection{Fase 3: Streaming Baja Latencia}
Conversión RVC/So-VITS-SVC con Information Bottleneck.
\begin{equation}
    Y_{str} = Dec(Content(X_{post}), F0_{smooth}, S_{pre})
\end{equation}

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 14}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo: Fase 3}
\end{figure}

\subsubsection{Fase 4: Modulación Emocional}
Integración de Speech Emotion Recognition (SER).

\begin{figure}[htbp]
    \centering
    \fbox{\begin{minipage}{0.9\columnwidth}
    \centering
    \vspace{0.5cm}
    \textbf{Aquí va mermaid 15}
    \vspace{0.5cm}
    \end{minipage}}
    \caption{Diagrama de flujo: Fase 4}
\end{figure}

% --- CONCLUSIONES ---
\section{Conclusiones Generales}
La evolución del proyecto permite establecer hallazgos críticos. La voz humana no puede tratarse como un fenómeno estático; la aproximación global resulta insuficiente. El éxito de la rehabilitación espectral depende de la capacidad de ajustar la señal en una escala temporal micro-segmentada.

Se evidenció una dicotomía entre limpieza de señal y fidelidad tímbrica. Los algoritmos de sustracción espectral (Wiener+VAD) están limitados a ruido estacionario. Un hallazgo fundamental fue la criticidad de la alineación temporal; cualquier desviación rítmica genera incoherencias de fase. El futuro apunta hacia la caracterización multidimensional empleando tensores y matrices de mayores dimensiones.

% --- REFERENCIAS ---
\begin{thebibliography}{00}

\bibitem{ref1} T. E. Oliphant, "A guide to NumPy," USA: Trelgol Publishing, vol. 1, 2006.
\bibitem{ref2} W. McKinney, "Python for data analysis: Data wrangling with Pandas, NumPy, and IPython," O'Reilly Media, Inc., 2012.
\bibitem{ref3} S. van der Walt, S. C. Colbert, and G. Varoquaux, "The NumPy array," \textit{Computing in Science \& Engineering}, vol. 13, no. 2, pp. 22-30, 2011.
\bibitem{ref4} J. M. Kizza, "Python for scientific computing," in \textit{Guide to Computer Network Security}, Springer, 2017.
\bibitem{ref5} P. Virtanen et al., "SciPy 1.0: fundamental algorithms for scientific computing in Python," \textit{Nature Methods}, vol. 17, no. 3, 2020.
\bibitem{ref6} E. Jones, T. Oliphant, and P. Peterson, "SciPy: Open source scientific tools for Python," 2001.
\bibitem{ref7} A. Savitzky and M. J. E. Golay, "Smoothing and differentiation of data," \textit{Analytical Chemistry}, vol. 36, no. 8, 1964.
\bibitem{ref8} R. W. Schafer, "What is a Savitzky-Golay filter?," \textit{IEEE Signal Processing Magazine}, vol. 28, no. 4, 2011.
\bibitem{ref9} W. H. Press and S. A. Teukolsky, "Savitzky-Golay smoothing filters," \textit{Computers in Physics}, vol. 4, no. 6, 1990.
\bibitem{ref10} M. Schmid et al., "Why and how Savitzky–Golay filters should be replaced," \textit{ACS Measurement Science Au}, vol. 2, no. 2, 2022.
\bibitem{ref11} H. H. Madden, "Comments on the Savitzky-Golay convolution method," \textit{Analytical Chemistry}, vol. 50, no. 9, 1978.
\bibitem{ref12} J. O. Smith, "Spectral audio signal processing," W3K Publishing, 2011.
\bibitem{ref13} L. R. Rabiner and B. Gold, "Theory and application of digital signal processing," Prentice-Hall, 1975.
\bibitem{ref14} J. B. Allen and L. R. Rabiner, "A unified approach to short-time Fourier analysis," \textit{Proc. IEEE}, vol. 65, 1977.
\bibitem{ref15} M. R. Portnoff, "Time-frequency representation of digital signals," \textit{IEEE Trans. Acoust., Speech, Signal Process.}, vol. 28, 1980.
\bibitem{ref16} M. Dolson, "The phase vocoder: A tutorial," \textit{Computer Music Journal}, vol. 10, no. 4, 1986.
\bibitem{ref17} D. Griffin and J. Lim, "Signal estimation from modified short-time Fourier transform," \textit{IEEE Trans. ASSP}, vol. 32, 1984.
\bibitem{ref18} B. Sharpe, "Invertibility of overlap-add processing," accessed July 2019.
\bibitem{ref19} L. R. Rabiner and R. W. Schafer, "Digital processing of speech signals," Prentice Hall, 1978.
\bibitem{ref20} Y. Ephraim and D. Malah, "Speech enhancement using a minimum-mean square error STSA estimator," \textit{IEEE Trans. ASSP}, vol. 32, 1984.
\bibitem{ref21} N. Wiener, "Extrapolation, interpolation, and smoothing of stationary time series," MIT Press, 1949.
\bibitem{ref22} J. Chen et al., "New insights into the noise reduction Wiener filter," \textit{IEEE Trans. Audio, Speech, Lang. Process.}, vol. 14, 2006.
\bibitem{ref23} P. C. Loizou, "Speech enhancement: theory and practice," CRC Press, 2013.
\bibitem{ref24} S. F. Boll, "Suppression of acoustic noise in speech using spectral subtraction," \textit{IEEE Trans. ASSP}, vol. 27, 1979.
\bibitem{ref25} J. Sohn et al., "A statistical model-based voice activity detection," \textit{IEEE Signal Process. Lett.}, vol. 6, 1999.
\bibitem{ref26} A. J. M. Houtsma, "Pitch and timbre," \textit{Journal of New Music Research}, vol. 26, 1997.
\bibitem{ref27} H. M. Teager and S. M. Teager, "Evidence for nonlinear sound production mechanisms," in \textit{Speech Production}, Springer, 1990.
\bibitem{ref28} P. Ladefoged, "Vowels and consonants," Blackwell Publishers, 2001.
\bibitem{ref29} G. Fant, "Acoustic theory of speech production," Mouton, 1960.
\bibitem{ref30} G. E. Peterson and H. L. Barney, "Control methods used in a study of the vowels," \textit{J. Acoust. Soc. Am.}, vol. 24, 1952.
\bibitem{ref31} J. Hillenbrand et al., "Acoustic characteristics of American English vowels," \textit{J. Acoust. Soc. Am.}, vol. 97, 1995.
\bibitem{ref32} K. N. Stevens, "Acoustic phonetics," MIT Press, 1998.
\bibitem{ref33} D. H. Whalen and A. G. Levitt, "The universality of intrinsic F0 of vowels," \textit{Journal of Phonetics}, vol. 23, 1995.
\bibitem{ref34} I. R. Titze, "Principles of voice production," National Center for Voice and Speech, 2000.
\bibitem{ref35} M. Hirano, "Clinical examination of voice," Springer, 2013.
\bibitem{ref36} C. E. Silver et al., "Current trends in initial management of laryngeal cancer," \textit{Eur. Arch. Otorhinolaryngol.}, vol. 266, 2009.
\bibitem{ref37} M. Remacle et al., "Endoscopic cordectomy. A proposal for a classification," \textit{Eur. Arch. Otorhinolaryngol.}, vol. 257, 2000.
\bibitem{ref38} E. V. Sjögren et al., "Voice outcome in T1a midcord glottic carcinoma," \textit{Arch. Otolaryngol.–Head Neck Surg.}, vol. 134, 2008.
\bibitem{ref39} T. Yılmaz et al., "Voice after cordectomy type I or type II," \textit{Otolaryngol.–Head Neck Surg.}, vol. 168, 2023.
\bibitem{ref40} L. M. Aaltonen et al., "Voice quality after treatment of early vocal cord cancer," \textit{Int. J. Radiat. Oncol. Biol. Phys.}, vol. 90, 2014.
\bibitem{ref41} H. S. Lee et al., "Voice outcome according to surgical extent," \textit{The Laryngoscope}, vol. 126, 2016.
\bibitem{ref42} A. K. Fouad et al., "Laryngeal compensation for voice production," \textit{Clin. Exp. Otorhinolaryngol.}, vol. 8, 2015.
\bibitem{ref43} G. Fant, "Acoustic theory of speech production: with calculations," Mouton, 1960.
\bibitem{ref44} T. Chiba and M. Kajiyama, "The vowel: Its nature and structure," Tokyo-Kaiseikan, 1941.
\bibitem{ref45} K. N. Stevens, "Acoustic phonetics," MIT Press, 1999.
\bibitem{ref46} J. L. Flanagan, "Speech analysis synthesis and perception," Springer, 1972.
% Añadir el resto de referencias...
\end{thebibliography}

\end{document}
